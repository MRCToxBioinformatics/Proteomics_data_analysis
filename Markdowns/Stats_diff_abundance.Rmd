---
title: "Differential abundance testing for quantitative MS"
author: "Tom Smith"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
  pdf_document: default
bibliography: bib.json
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```


There are a number of statistical tests/R packages one can use to perform differential abundance testing for proteomics data. The list below is by no means complete:

- **t-test**: If we assume that the quantification values are Gaussian
distributed, a t-test may be appropriate. For LFQ/TMT, log-transformed abundances can be assumed to be Gaussian distributed. For SILAC, log-transformed ratios can be assumed to be Gaussian distributed.
When we have one condition variable in a SILAC experiment, and we are comparing between two values
(e.g SILAC ratio is a comparison between treatment and control), a one-sample t-test is appropriate. When we have one condition variable and we are comparing between two values variable in an TMT/LFQ (e.g samples are treatment or control), a two-sample t-test is appropriate. Similarly, if the SILAC ratio captures one condition variable, with another condition variable across samples (e.g ratio is treatment vs control and samples are from two different cell lines), a two-sample t-test is appropriate.

- **ANOVA/linear model**: Where a more complex experimental design is involved,
an ANOVA or linear model can be used, on the same assumptions at the t-test.

- **`limma`** [@http://zotero.org/users/5634351/items/6KTXTWME]:
Proteomics experiments are typically lowly replicated (e.g n << 10).
Variance estimates are therefore inaccurate. `limma`  is an R package that extends
the t-test/ANOVA/linear model testing framework to enable sharing of information
across features (here, proteins) to update the variance estimates. This decreases
false positives and increases statistical power.

- **`DEqMS`** [@http://zotero.org/users/5634351/items/RTM6NFVU]: limma assumes there is a relationship between protein abundance and
variance. This is usually the case, although
  - The relationship with variance is often stronger with the number of
  peptide spectrum matches (for TMT experiments), or peptides (for SILAC/LFQ experiments)
  - For SILAC, the quantification value is the ratio, not protein abundance.
  There is no reason to believe there will be a relationship between the mean ratio and variance
  
  As such, `DEqMS` should be preferred over limma for all quantitative proteomics data.

Here, we will perform statistical analyses on SILAC, LFQ and TMT quantitative data.
These are examples only and the code herein is unlikely to be directly applicable
to your own dataset.

### Load dependencies

Load the required libraries.

```{r, message=FALSE, warning=FALSE}
library(camprotR)
library(ggplot2)
library(MSnbase)
library(DEqMS)
library(limma)
library(dplyr)
library(tidyr)
library(ggplot2)
library(broom)
library(biobroom)

```


### SILAC

Here, we will start with the SILAC data processed in [Processing and QC of SILAC data](https://mrctoxbioinformatics.github.io/Proteomics_data_analysis/Markdowns/SILAC.html).
Please see the previous notebook for details of the experimental design and aim and data processing. 

In brief, we wish to determine the proteins which are significantly enriched by UV crosslinking (CL) vs non-CL control (NC).

We will use two approaches:
- one sample t-test
- moderated one sample t-test (DEqMS)

First, we read in the protein-level ratios obtained in the above notebooks. We will filter out proteins which are not present in at least 3/4 replicates.
```{r}
silac_protein <- readRDS('results/prot_res.rds') %>%
  filterNA(pNA = 1/4) # max 1/4 missing.
```


#### T-test 
To perform a t-test for each protein, we want to extract the quantifications in a long 'tidy' format. We can do this using the biobroom package
```{r}
silac_protein_tidy <- biobroom::tidy.MSnSet(silac_protein) %>%
  filter(is.finite(value))
```

As an example of how to run a single t-test, let's subset to a single protein.
First, we extrac the quantification values for this single protein
```{r}

example_protein <- 'O00159'

silac_protein_tidy_example <- silac_protein_tidy %>%
  filter(protein==example_protein)

print(silac_protein_tidy_example)
```

Then we use `t.test` to perform the t-test. Since we are giving only a single
vector of values, a one sample t-test is performed.

```{r}
t.test.res <- t.test(silac_protein_tidy_example$value,
                     alternative='two.sided')

print(t.test.res)


```

We can use `tidy` from the `broom` package to return the t-test results in
a tidy tibble. The value of this will be seen in the next code chunk.

```{r}
tidy(t.test.res)
```

We can now apply a t-test to every protein using dplyr `group` and `do`, making use of `tidy`.


```{r}
t.test.res.all <- silac_protein_tidy %>%
  group_by(protein) %>%
  do(tidy(t.test(.$value, alternative='two.sided')))
```

Here are the results for the t-test for the example protein. As we can see, the 'estimate' column in `t.text.res.all` is the mean log2 ratio. The 'statistic' column is the t-statistic and the 'parameter' column is the degrees of freedom for the t-statistic. All the values are identical since have performed the exact same test with both approaches.
```{r}
print(t.test.res)
t.test.res.all %>% filter(protein==example_protein)

```
When you are performing a lot of statistical tests at the same time, it's recommended practise to plot the p-value distribution. If the assumptions of the test are valid, one expects a uniform distribution from 0-1 for those tests where the null hypothesis should not be rejected. Statistically significant tests will show as a peak of very low p-values. If there are very clear skews in the uniform distribution, or strange peaks other than in the smallest p-value bin, that may indicate the assumptions of the test are not valid, for some or all tests. 

Here, we have so many significant tests that the uniform distribution is hard to assess. Note that, beyond the clear peak for very low p-values (<0.05) we also have a slight skew towards low p-values in the range 0.05-0.15. This may indicate insufficient statistical power to detect some proteins that are truly enriched upon CL.  
```{r}
hist(t.test.res.all$p.value, 20)
```

Since we have performed multiple tests, we want to calculate an adjusted p-value
to avoid type I errors (false positives).

Here, are using the Benjamini, Y., and Hochberg, Y. (1995) method to estimate the
False Discovery Rate, e.g the proportion of false positives amongst the rejected null hypotheses.


```{r}
? p.adjust
t.test.res.all$padj <- p.adjust(t.test.res.all$p.value, method='BH')
table(t.test.res.all$padj<0.01)
```

At an FDR of 1%, we have `r sum(t.test.res.all$padj<0.01)` proteins with a significant difference.



#### Moderated t-test (DEqMS)

To identify proteins with significantly increased abundance in CL vs NC, we will
use DEqMS [@http://zotero.org/users/5634351/items/RTM6NFVU], which you can think
of as an extension of limma [@http://zotero.org/users/5634351/items/6KTXTWME]
specifically for proteomics. 

The analysis steps are taken from the
[DEqMS vignette](https://bioconductor.org/packages/release/bioc/vignettes/DEqMS/inst/doc/DEqMS-package-vignette.html#extract-quant-data-columns-for-deqms).
We first want to create an `MArrayLM` object as per normal `limma` analysis and 
then  add a `$count` column to the `MArrayLM` object and use the `spectraCounteBayes`
function to perform the Bayesian shrinkage using the count column, which describes
the number of pepitdes per protein. This is contrast to `limma`, which uses the
`$Amean` column, which describes the mean CL:NC ratio.

First, we create the `MArrayLM` object. 
```{r}
dat <- silac_protein  %>% 
  exprs()

# Performing the equivalent of a one-sample t-test, so the design matrix is just an intercept
design <- cbind(Intercept = rep(1, ncol(dat))) 

fit <- lmFit(dat, design)
efit <- eBayes(fit)
```

Next, we define the `$count` column, which `DEqMS` will use. In the DEqMS paper,
they suggest that the best summarisation metric to use is the minimum value across
the samples, so our `count` column is the minimum number of peptides per protein.  
```{r}
protein_ratio_long <- silac_protein %>%
  exprs() %>%
  data.frame() %>%
  tibble::rownames_to_column('Master.Protein.Accessions') %>%
  pivot_longer(cols=-Master.Protein.Accessions, values_to='protein_ratio', names_to='sample')

pep_res <- readRDS('results/pep_res.rds')

# Obtain the min peptide count across the samples and determine the minimum value across 
# samples
min_pep_count <- camprotR::count_features_per_protein(pep_res) %>%
  merge(protein_ratio_long, by=c('Master.Protein.Accessions', 'sample')) %>%
  filter(is.finite(protein_ratio)) %>%  # We only want to consider samples with a ratio quantified
  group_by(Master.Protein.Accessions) %>%
  summarise(min_pep_count = min(n))

# add the min peptide count
efit$count <- min_pep_count$min_pep_count
```

And now we run `spectraCounteBayes` from `DEqMS` to perform the statistical test.
```{r}
# run DEqMS
efit_deqms <- suppressWarnings(spectraCounteBayes(efit))
```



Below, we inspect the peptide count vs variance relationship which `DEqMS` is 
using in the statistical test.

In this case the relationship between peptide count and variance is only really
apparent when the minimum number of peptides from which a protein-level ratio is 
obtained is very low. Typically, one might remove quantifications where there is 
just a single peptide for a protein. Here, we have kept them, and we can refer to 
the `count` column in the final results object if we want to check the minimum 
number of peptides observed per sample.

```{r, fig.height=5, fig.width=5}
# Diagnostic plots
VarianceBoxplot(efit_deqms, n = 30, xlab = "Peptides")
```


Below, we summarise the number of proteins with statistically different abundance 
in CL vs NC and plot a 'volcano' plot to visualise this. Note that all proteins 
with a statistically significant change are increased in CL, so the plot looks 
more like a fire-hose than a volcano!

```{r, fig.height=6, fig.width=6}
deqms_results <- outputResult(efit_deqms, coef_col = 1)

deqms_results %>%
  ggplot(aes(x = logFC, y = -log10(sca.P.Value), colour = sca.adj.pval < 0.01)) +
  geom_point() +
  theme_camprot(border=FALSE, base_size=15) +
  scale_colour_manual(values = c('grey', get_cat_palette(2)[2]), name = 'CL vs NC Sig.') +
  labs(x = 'CL vs NC (Log2)', y = '-log10(p-value)')
```
Finally, we can explore and, if we wish, export our results. Importantly, the `$t`, `$P.Value` and `$adj.P.Val` columnts are from `limma`. The columns prefixed with `sca` are the from `DEqMS`. 

```{r}
head(deqms_results)

```
We can now compare the results from the t-test and the moderate t-test (DEqMS).
Below, we update the column names so it's easier to see which column comes from which test.
```{r}
colnames(t.test.res.all) <- paste0('t.test.', colnames(t.test.res.all))
colnames(deqms_results) <- paste0('deqms.', colnames(deqms_results))

```

And then merge the two test results.
```{r}
silac_compare_tests <- merge(deqms_results,
      t.test.res.all,
      by.x='row.names',
      by.y='t.test.protein')

```

> ### Exercise
> Compare the effect size estimates from the two methods with a scatter plot.
> Can you explain what you observe?
>
> Hints:
>
> - For the t-test, you want the 't.test.estimate' column
> - For the moderated t-test, you want the 'deqms.logFC' column


### Solution
```{r class.source = 'fold-hide', eval=FALSE, fig.show='hide'}
ggplot(silac_compare_tests) +
  aes(t.test.estimate, deqms.logFC) +
  geom_point() +
  geom_abline(slope=1) +
  theme_camprot(border=FALSE) +
  labs(x='t.test mean ratio', y='DEqMS mean ratio')
```
We can also compare the p-values from the two tests. Note that the p-value is almost
always lower for the moderated t-test with DEqMS than the standard t-test.

```{r}
p <- ggplot(silac_compare_tests) +
  aes(log10(t.test.p.value), log10(deqms.P.Value)) +
  geom_point() +
  geom_abline(slope=1, linetype=2, colour=get_cat_palette(1), size=1) +
  theme_camprot(border=FALSE) +
  labs(x='T-test log10(p-value)', y='DEqMS log10(p-value)')

print(p)
```

Finally, we can compare the number of proteins with a significant difference
(Using 1% FDR threshold) according to each test. Using DEqMS more than doubles the number
of null hypothesis rejections. Only 3 proteins are signficant with a t-test but not the moderated t-test.
```{r}
silac_compare_tests %>%
  group_by(t.test.padj<0.01,
           deqms.sca.adj.pval<0.01) %>%
  tally()


```

> ### Extended Exercise (Optional)
> Compare the number of significant differences (at 1% FDR) for each method,
> grouping by the number of peptides per protein. An example of how to visualise 
> this is below, though you do not need to replicate this visualisation precisely!
> 
> How do you interpet this?
>
> Hints:
>
> - For the number of peptides, you want the deqms.count column. You need
>   to bin this into reasonable bins. See e.g ?Hmisc::cut2 for this.


### Solution
```{r class.source = 'fold-hide', eval=FALSE}

# Start by tallying as in code chunk above
silac_compare_tests %>%
  group_by('min_peptides'=Hmisc::cut2(deqms.count, cuts=c(1:4,10)),
           't-test'=t.test.padj<0.01,
           'deqms'=deqms.sca.adj.pval<0.01) %>%
  tally() %>%
  
  # make a single column describing yes/no signmificant with each method and 
  # rename to make it more intuitive
  mutate(compare_tests=recode(interaction(`t-test`, deqms), 
                              'FALSE.FALSE'='Neither',
                              'TRUE.TRUE'='Both',
                              'TRUE.FALSE'='t-test',
                              'FALSE.TRUE'='DEqMS')) %>%
  mutate(compare_tests=factor(compare_tests, levels=c('t-test', 'DEqMS', 'Both', 'Neither'))) %>%
  
  ggplot(aes(min_peptides, n, fill=compare_tests)) +
  geom_bar(stat='identity',position='fill') +
  geom_text(position=position_fill(vjust=0.5), aes(label=n)) +
  theme_camprot(border=FALSE, base_size=15) +
  scale_fill_manual(values=c(get_cat_palette(3), 'grey'), name='') +
  labs(x='Minimum peptides per sample', y='Fraction of proteins')

# Note that the 3 proteins which are only significant with the t-test have 
```


## LFQ 
```{r}
lfq_protein <- readRDS('./results/lfq_prot_robust.rds')
```


First, we need to calculate the RNase +/- ratios, since this is the value we want to normalise in this case.
```{r}
ratios <- exprs(lfq_protein[,pData(lfq_protein)$Condition == 'RNase_neg']) - 
  exprs(lfq_protein[,pData(lfq_protein)$Condition == 'RNase_pos'])

lfq_protein_ratios <- MSnSet(exprs = ratios,
                             fData = fData(lfq_protein),
                             pData = (pData(lfq_protein) %>% filter(Condition == 'RNase_neg')))

print(nrow(lfq_protein_ratios))

lfq_protein_ratios <- lfq_protein_ratios %>% filterNA(pNA=1/4)

print(nrow(lfq_protein_ratios))
```

```{r}
lfq_protein_tidy <- lfq_protein_ratios %>%
  biobroom::tidy.MSnSet() %>%
  filter(is.finite(value))
  
example_protein <- 'A5YKK6'

lfq_protein_tidy_example <- lfq_protein_tidy %>% filter(protein==example_protein)

lfq.t.test.example <- t.test(lfq_protein_tidy_example$value,
       alternative='two.sided',
       var.equal=FALSE,
       paired=FALSE)

print(lfq.t.test.example)
```
```{r}
lfq.t.test.all <- lfq_protein_tidy %>%
  group_by(protein) %>%
  do(tidy(t.test(value ~ 1, data = ., alternative='two.sided', var.equal=FALSE, paired=FALSE)))
```

Here are the results for the t-test for the example protein. As we can see, the 'estimate' column in `t.text.res.all` is the mean log2 ratio. The 'statistic' column is the t-statistic and the 'parameter' column is the degrees of freedom for the t-statistic. All the values are identical since have performed the exact same test with both approaches.
```{r}
print(lfq.t.test.example)
lfq.t.test.all %>% filter(protein==example_protein)

```

```{r}
hist(lfq.t.test.all$p.value)
```

```{r}
lfq.t.test.all$padj <- p.adjust(lfq.t.test.all$p.value, method='BH')
```

```{r}
lfq.t.test.all
```




## TMT

First, we read in the protein-level quantification generated in the notebook linked above
```{r}
tmt_protein <- readRDS('./results/tmt_protein.rds')
```

As a reminder, this data comes from a published benchmark experiment where yeast peptides were spiked into human peptides at 3 known amounts to provide ground truth fold changes (see below). Two versions of Orbitrap control software were tested, with v3 shown to improve fold-change estimation for low intensity peptides. Here, we will use the data obtained with v3. For more details, see: [@http://zotero.org/users/5634351/items/LG3W8G4T]

<img src="https://github.com/CambridgeCentreForProteomics/notch/blob/master/images/toc.png?raw=true" width="70%"/>

To keep things simple, we will just focus on the comparison between the 2x and 6x yeast spike-in samples (the last 6 TMT tags).

```{r}
tmt_protein <- tmt_protein[,1:7]
print(pData(tmt_protein))
```

